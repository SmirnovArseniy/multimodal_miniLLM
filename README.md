# Задание multimodal-llm с курса Глубинное Обучение (ФТиАД 2024)
LLM обучена понимать модальность аудио.

Реализована архитектура аналогичная LLaVA ([github](https://github.com/haotian-liu/LLaVA), [paper](https://arxiv.org/abs/2304.08485))
но для аудио.

Обучен адаптер аудио для LLM по аналогии с LLava

В качестве LLM использован [TinyLlama](https://github.com/jzhang38/TinyLlama).

В качестве CLIP-like энкодера аудио использован CLIP-like экнодер для 5 модальностей.

Идея и метод обучения точно такой же как у CLIP.
Из нового: это больше данных разных модальностей. Крутость модельки в том, что через модальность изображений, получилось обучить совместные эмбэддинги для всех модальностей. То есть можно сделать zero-shot классификатор для Audio, хотя модальность текста и аудио ни разу во время обучения не взаимодействовали - все обучалось через изображения (то есть изображение-аудио / изображение-текст).
</br>

## Структура проекта:</br>

- Ноутбук с построением, обучением, инференсом кастомной LLaVa-модели
- Итоговая модель: **лучший скор BLEU на валидации: 6.6**
</br>